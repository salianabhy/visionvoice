# üëÅÔ∏è VisionVoice ‚Äî AI Image Describer for the Visually Impaired

A full-stack college AI project that lets users upload an image and **hear a spoken description** of what's in it ‚Äî powered by BLIP (Salesforce) for image captioning and gTTS for text-to-speech.

---

## üóÇÔ∏è Project Structure

```
visionvoice/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ app.py              ‚Üê Flask API server (main entry point)
‚îÇ   ‚îú‚îÄ‚îÄ model_loader.py     ‚Üê Loads and runs the BLIP AI model
‚îÇ   ‚îú‚îÄ‚îÄ tts_generator.py    ‚Üê Converts text to MP3 using gTTS
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt    ‚Üê Python dependencies
‚îÇ   ‚îî‚îÄ‚îÄ static/
‚îÇ       ‚îî‚îÄ‚îÄ audio/          ‚Üê Generated MP3 files saved here
‚îÇ
‚îî‚îÄ‚îÄ frontend/
    ‚îú‚îÄ‚îÄ index.html
    ‚îú‚îÄ‚îÄ package.json
    ‚îú‚îÄ‚îÄ vite.config.js
    ‚îî‚îÄ‚îÄ src/
        ‚îú‚îÄ‚îÄ main.jsx
        ‚îú‚îÄ‚îÄ App.jsx           ‚Üê Root component + API call logic
        ‚îú‚îÄ‚îÄ App.css           ‚Üê All styles
        ‚îî‚îÄ‚îÄ components/
            ‚îú‚îÄ‚îÄ ImageUpload.jsx      ‚Üê Drag & drop uploader + preview
            ‚îú‚îÄ‚îÄ DescriptionResult.jsx ‚Üê Shows description + audio player
            ‚îî‚îÄ‚îÄ AudioPlayer.jsx      ‚Üê Custom audio controls
```

---

## ‚öôÔ∏è How to Set Up & Run Locally

### Prerequisites
- **Python 3.9+** installed
- **Node.js 18+** installed
- Internet connection (model downloads ~1GB on first run)

---

### Step 1 ‚Äî Set up the Python Backend

Open a terminal and navigate to the backend folder:

```bash
cd visionvoice/backend
```

Create a virtual environment (recommended):

```bash
python -m venv venv

# On Windows:
venv\Scripts\activate

# On Mac/Linux:
source venv/bin/activate
```

Install dependencies:

```bash
pip install -r requirements.txt
```

Start the Flask server:

```bash
python app.py
```

You should see:
```
Loading BLIP model... (this may take a minute on first run)
Model loaded successfully on cpu!
Backend ready! Listening for requests...
 * Running on http://0.0.0.0:5000
```

> ‚è≥ **First run takes 1‚Äì2 minutes** to download the BLIP model (~1GB). After that it loads in seconds from cache.

---

### Step 2 ‚Äî Set up the React Frontend

Open a **second terminal** and navigate to the frontend folder:

```bash
cd visionvoice/frontend
```

Install Node.js dependencies:

```bash
npm install
```

Start the React development server:

```bash
npm run dev
```

You should see:
```
  VITE v5.x.x  ready

  ‚ûú  Local:   http://localhost:3000/
```

---

### Step 3 ‚Äî Use the App

1. Open your browser and go to **http://localhost:3000**
2. Upload any image by dragging & dropping or clicking the upload area
3. Click **"Describe Image"**
4. Wait 5‚Äì15 seconds for the AI to analyze it
5. Read the description and click ‚ñ∂ to **hear it spoken aloud**

---

## ü§ñ How the AI Works

```
User uploads image
        ‚Üì
Frontend (React) sends image via POST /describe-image
        ‚Üì
Flask backend receives image
        ‚Üì
BLIP model (Salesforce/blip-image-captioning-base) generates text description
        ‚Üì
gTTS converts text ‚Üí MP3 audio file
        ‚Üì
Flask returns { description, audio_url }
        ‚Üì
Frontend displays text + plays audio automatically
```

---

## üß∞ Tech Stack

| Layer       | Technology                                      |
|-------------|------------------------------------------------|
| Frontend    | React 18, Vite, CSS3                           |
| Backend     | Python, Flask, Flask-CORS                      |
| AI Model    | BLIP (Salesforce/blip-image-captioning-base)   |
| AI Library  | HuggingFace Transformers, PyTorch              |
| Image Proc. | Pillow (PIL)                                   |
| Speech      | gTTS (Google Text-to-Speech)                   |
| API Style   | REST (JSON over HTTP)                          |

---

## üéì Demo Tips (For Class Presentation)

1. **Pre-load the model** ‚Äî Start the backend 5 minutes before your demo so the model is warmed up
2. **Use clear images** ‚Äî A dog in a park, a person cooking, a car on a road all work great
3. **Show the flow** ‚Äî Open DevTools Network tab to show the actual POST request happening
4. **Accessibility angle** ‚Äî Point out the high-contrast design, large buttons, and keyboard navigation
5. **Offline audio** ‚Äî The "Download Audio" button lets you save and play the MP3 offline

---

## üîß Troubleshooting

| Problem | Solution |
|---------|---------|
| Model download stuck | Check your internet connection; it's ~1GB |
| `CORS error` in browser | Make sure Flask is running on port 5000 |
| Audio won't play | Browser may block autoplay; click the ‚ñ∂ button manually |
| `ModuleNotFoundError` | Make sure you activated the virtual environment |
| Slow first response | Normal ‚Äî BLIP model needs ~5‚Äì15 sec on CPU |

---

## üìö Acknowledgements

- [BLIP Model](https://huggingface.co/Salesforce/blip-image-captioning-base) by Salesforce Research
- [gTTS](https://gtts.readthedocs.io/) by Pierre Nicolas Durette
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)
